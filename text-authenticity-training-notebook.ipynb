{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìö Text Authenticity Detection - Training Notebook\n",
    "\n",
    "## Welcome to Your First NLP Machine Learning Project!\n",
    "\n",
    "In this notebook, we'll learn how to build a machine learning model to detect fake text. This is a practical introduction to:\n",
    "- **Natural Language Processing (NLP)**\n",
    "- **Feature Engineering**\n",
    "- **Machine Learning Classification**\n",
    "- **Model Evaluation**\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ The Problem\n",
    "\n",
    "**Goal**: Given two text articles, determine which one is real and which one is AI-generated (fake).\n",
    "\n",
    "**Why is this important?**\n",
    "- Combating misinformation\n",
    "- Protecting against AI-generated spam\n",
    "- Understanding how AI-generated text differs from human writing\n",
    "\n",
    "**What makes this challenging?**\n",
    "- AI text is getting very sophisticated\n",
    "- Subtle differences in writing style\n",
    "- Need to extract meaningful features from text\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ Learning Path\n",
    "\n",
    "We'll build our solution in these steps:\n",
    "1. **Explore the Data** - Understanding what we're working with\n",
    "2. **Extract Simple Features** - Convert text to numbers\n",
    "3. **Build a Simple Model** - Start with basic machine learning\n",
    "4. **Evaluate Performance** - How good is our model?\n",
    "5. **Improve Features** - Add more sophisticated text analysis\n",
    "6. **Try Better Models** - Ensemble methods\n",
    "7. **Advanced Techniques** - For those ready for more!\n",
    "\n",
    "Let's begin! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries and Setup\n",
    "\n",
    "First, let's import the tools we'll need. Don't worry if you don't recognize all of these - we'll explain them as we go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic data manipulation\n",
    "import pandas as pd  # For working with data tables\n",
    "import numpy as np   # For numerical operations\n",
    "import os           # For file operations\n",
    "\n",
    "# Text processing\n",
    "import re           # For text pattern matching\n",
    "from collections import Counter  # For counting things\n",
    "\n",
    "# Visualization - helps us understand our data\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('default')\n",
    "\n",
    "# Text analysis tools\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from textstat import flesch_reading_ease  # Measures how easy text is to read\n",
    "\n",
    "# Machine Learning tools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.data.find('vader_lexicon')\n",
    "except LookupError:\n",
    "    nltk.download('vader_lexicon')\n",
    "\n",
    "print(\"‚úÖ All libraries loaded successfully!\")\n",
    "print(\"Let's start learning about text authenticity detection!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Understanding Our Data\n",
    "\n",
    "Before building any model, we need to understand what data we have. This is called **Exploratory Data Analysis (EDA)**.\n",
    "\n",
    "### üìÅ Data Structure\n",
    "Our data is organized like this:\n",
    "```\n",
    "data/\n",
    "‚îú‚îÄ‚îÄ train.csv          # Tells us which text is real for each article\n",
    "‚îú‚îÄ‚îÄ train/             # Training text files\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ article_0000/\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ file_1.txt # One of these is real...\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ file_2.txt # ...and one is AI-generated\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ article_0001/\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ file_1.txt\n",
    "‚îÇ       ‚îî‚îÄ‚îÄ file_2.txt\n",
    "‚îî‚îÄ‚îÄ test/              # Test files (we predict these)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up our file paths\n",
    "# üîß MODIFY THESE PATHS for your data location\n",
    "BASE_PATH = '/kaggle/input/fake-or-real-the-impostor-hunt/data'\n",
    "TRAIN_CSV = f'{BASE_PATH}/train.csv'\n",
    "TRAIN_DIR = f'{BASE_PATH}/train'\n",
    "TEST_DIR = f'{BASE_PATH}/test'\n",
    "\n",
    "# For reproducible results\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(f\"üìÅ Looking for data in: {BASE_PATH}\")\n",
    "\n",
    "# Let's see what we have\n",
    "if os.path.exists(TRAIN_CSV):\n",
    "    print(\"‚úÖ Found training labels file\")\n",
    "else:\n",
    "    print(\"‚ùå Training labels file not found - check your paths!\")\n",
    "    \n",
    "if os.path.exists(TRAIN_DIR):\n",
    "    print(\"‚úÖ Found training text files\")\n",
    "else:\n",
    "    print(\"‚ùå Training directory not found - check your paths!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Let's Look at the Labels\n",
    "\n",
    "The CSV file tells us which text (file_1.txt or file_2.txt) is the real one for each article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the labels\n",
    "if os.path.exists(TRAIN_CSV):\n",
    "    labels_df = pd.read_csv(TRAIN_CSV)\n",
    "    \n",
    "    print(f\"üìä We have labels for {len(labels_df)} articles\")\n",
    "    print(\"\\nFirst few rows:\")\n",
    "    print(labels_df.head())\n",
    "    \n",
    "    print(\"\\nüéØ What the columns mean:\")\n",
    "    print(\"‚Ä¢ 'id': Article number\")\n",
    "    print(\"‚Ä¢ 'real_text_id': Which file is real (1 = file_1.txt, 2 = file_2.txt)\")\n",
    "    \n",
    "    # Let's see the distribution\n",
    "    real_text_counts = labels_df['real_text_id'].value_counts()\n",
    "    print(f\"\\nüìà Distribution of real texts:\")\n",
    "    print(f\"file_1.txt is real: {real_text_counts.get(1, 0)} times\")\n",
    "    print(f\"file_2.txt is real: {real_text_counts.get(2, 0)} times\")\n",
    "    \n",
    "    # Is our dataset balanced?\n",
    "    balance_ratio = min(real_text_counts) / max(real_text_counts)\n",
    "    if balance_ratio > 0.8:\n",
    "        print(\"‚úÖ Dataset is well balanced!\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Dataset is imbalanced - we might need to handle this\")\n",
    "else:\n",
    "    print(\"‚ùå Cannot analyze labels - file not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìö Reading the Actual Text Files\n",
    "\n",
    "Now let's write a function to load the actual text content. This is a common pattern in data science - write reusable functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text_data(train_dir, train_csv_path):\n",
    "    \"\"\"\n",
    "    Load text data and combine with labels.\n",
    "    \n",
    "    This function demonstrates:\n",
    "    - File handling\n",
    "    - Data organization\n",
    "    - Error handling (try/except)\n",
    "    \"\"\"\n",
    "    print(f\"üìñ Loading text data...\")\n",
    "    \n",
    "    # Load the labels first\n",
    "    if not os.path.exists(train_csv_path):\n",
    "        print(f\"‚ùå Labels file not found: {train_csv_path}\")\n",
    "        return None\n",
    "    \n",
    "    train_df = pd.read_csv(train_csv_path)\n",
    "    print(f\"üìä Found labels for {len(train_df)} articles\")\n",
    "    \n",
    "    # Now load the text files\n",
    "    texts_data = []\n",
    "    successful_loads = 0\n",
    "    \n",
    "    for idx, row in train_df.iterrows():\n",
    "        article_id = row['id']\n",
    "        real_text_id = row['real_text_id']\n",
    "        \n",
    "        # Build file paths\n",
    "        article_dir = os.path.join(train_dir, f\"article_{article_id:04d}\")\n",
    "        text_1_path = os.path.join(article_dir, \"file_1.txt\")\n",
    "        text_2_path = os.path.join(article_dir, \"file_2.txt\")\n",
    "        \n",
    "        # Try to read both files\n",
    "        try:\n",
    "            with open(text_1_path, 'r', encoding='utf-8') as f:\n",
    "                text_1 = f.read().strip()\n",
    "            with open(text_2_path, 'r', encoding='utf-8') as f:\n",
    "                text_2 = f.read().strip()\n",
    "            \n",
    "            # Organize the data\n",
    "            texts_data.append({\n",
    "                'article_id': article_id,\n",
    "                'text_1': text_1,\n",
    "                'text_2': text_2,\n",
    "                'real_text_id': real_text_id,\n",
    "                'real_text': text_1 if real_text_id == 1 else text_2,\n",
    "                'fake_text': text_2 if real_text_id == 1 else text_1\n",
    "            })\n",
    "            successful_loads += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            if successful_loads == 0:  # Only show first few errors\n",
    "                print(f\"‚ö†Ô∏è Couldn't load article {article_id}: {e}\")\n",
    "    \n",
    "    if successful_loads == 0:\n",
    "        print(\"‚ùå No text files loaded successfully\")\n",
    "        return None\n",
    "        \n",
    "    df = pd.DataFrame(texts_data)\n",
    "    print(f\"‚úÖ Successfully loaded {len(df)} text pairs\")\n",
    "    \n",
    "    if len(df) < len(train_df):\n",
    "        print(f\"‚ö†Ô∏è Only loaded {len(df)}/{len(train_df)} articles\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load our data\n",
    "text_df = load_text_data(TRAIN_DIR, TRAIN_CSV)\n",
    "\n",
    "if text_df is not None:\n",
    "    print(f\"\\nüìä Final dataset shape: {text_df.shape}\")\n",
    "    print(f\"Columns: {list(text_df.columns)}\")\n",
    "else:\n",
    "    print(\"‚ùå Failed to load data - check your file paths!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üëÄ Let's Look at Some Examples\n",
    "\n",
    "The best way to understand text data is to read some examples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if text_df is not None and len(text_df) > 0:\n",
    "    # Pick a random example\n",
    "    sample_idx = 0  # Let's start with the first one\n",
    "    sample = text_df.iloc[sample_idx]\n",
    "    \n",
    "    print(f\"üîç Example Article {sample['article_id']}\")\n",
    "    print(f\"Real text is: file_{sample['real_text_id']}.txt\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(\"üìÑ REAL TEXT (first 300 characters):\")\n",
    "    print(sample['real_text'][:300] + \"...\\n\")\n",
    "    \n",
    "    print(\"ü§ñ FAKE TEXT (first 300 characters):\")\n",
    "    print(sample['fake_text'][:300] + \"...\\n\")\n",
    "    \n",
    "    print(\"ü§î DISCUSSION QUESTIONS:\")\n",
    "    print(\"‚Ä¢ Can you spot any differences in writing style?\")\n",
    "    print(\"‚Ä¢ Which text seems more natural to you?\")\n",
    "    print(\"‚Ä¢ What patterns might help a computer tell them apart?\")\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(f\"\\nüìè Basic Stats:\")\n",
    "    print(f\"Real text length: {len(sample['real_text'])} characters\")\n",
    "    print(f\"Fake text length: {len(sample['fake_text'])} characters\")\n",
    "    print(f\"Real text words: {len(sample['real_text'].split())} words\")\n",
    "    print(f\"Fake text words: {len(sample['fake_text'].split())} words\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No data to show examples from\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Data Overview\n",
    "\n",
    "Let's get a high-level view of our dataset. This helps us understand what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if text_df is not None and len(text_df) > 0:\n",
    "    # Calculate basic statistics\n",
    "    real_lengths = [len(text) for text in text_df['real_text']]\n",
    "    fake_lengths = [len(text) for text in text_df['fake_text']]\n",
    "    \n",
    "    real_word_counts = [len(text.split()) for text in text_df['real_text']]\n",
    "    fake_word_counts = [len(text.split()) for text in text_df['fake_text']]\n",
    "    \n",
    "    print(\"üìä DATASET OVERVIEW\")\n",
    "    print(\"=\" * 30)\n",
    "    print(f\"Total articles: {len(text_df)}\")\n",
    "    print(f\"Real text ID distribution: {text_df['real_text_id'].value_counts().to_dict()}\")\n",
    "    \n",
    "    print(\"\\nüìè TEXT LENGTHS (characters):\")\n",
    "    print(f\"Real text - Average: {np.mean(real_lengths):.0f}, Range: {min(real_lengths)}-{max(real_lengths)}\")\n",
    "    print(f\"Fake text - Average: {np.mean(fake_lengths):.0f}, Range: {min(fake_lengths)}-{max(fake_lengths)}\")\n",
    "    \n",
    "    print(\"\\nüìù WORD COUNTS:\")\n",
    "    print(f\"Real text - Average: {np.mean(real_word_counts):.0f}, Range: {min(real_word_counts)}-{max(real_word_counts)}\")\n",
    "    print(f\"Fake text - Average: {np.mean(fake_word_counts):.0f}, Range: {min(fake_word_counts)}-{max(fake_word_counts)}\")\n",
    "    \n",
    "    # Are there obvious differences?\n",
    "    length_diff = abs(np.mean(real_lengths) - np.mean(fake_lengths))\n",
    "    word_diff = abs(np.mean(real_word_counts) - np.mean(fake_word_counts))\n",
    "    \n",
    "    print(\"\\nüîç FIRST OBSERVATIONS:\")\n",
    "    if length_diff > 100:\n",
    "        print(f\"‚Ä¢ Text lengths differ significantly ({length_diff:.0f} characters on average)\")\n",
    "    else:\n",
    "        print(\"‚Ä¢ Text lengths are similar - length alone won't distinguish them\")\n",
    "        \n",
    "    if word_diff > 20:\n",
    "        print(f\"‚Ä¢ Word counts differ significantly ({word_diff:.0f} words on average)\")\n",
    "    else:\n",
    "        print(\"‚Ä¢ Word counts are similar - we need more sophisticated features\")\n",
    "        \n",
    "    print(\"\\nüí° This tells us we need to look beyond simple length metrics!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No data available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì Checkpoint: What We've Learned So Far\n",
    "\n",
    "**Key Concepts:**\n",
    "1. **Exploratory Data Analysis (EDA)** - Understanding your data before modeling\n",
    "2. **File handling** - Reading data from multiple sources\n",
    "3. **Data organization** - Structuring data for analysis\n",
    "\n",
    "**Questions to Consider:**\n",
    "- What patterns did you notice between real and fake text?\n",
    "- What features might help distinguish them?\n",
    "- How would you approach this problem?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## üéâ Congratulations!\n\nYou've successfully built a machine learning system for text authenticity detection! \n\n### üèÜ **What You've Learned:**\n- **Data Science Fundamentals**: Loading, exploring, and understanding text data\n- **Feature Engineering**: Converting text into meaningful numerical features  \n- **Machine Learning**: Training and evaluating classification models\n- **Model Interpretation**: Understanding what your model learned\n- **Scientific Thinking**: Comparing approaches and drawing conclusions\n\n### üåü **Next Steps:**\n- **Practice**: Try this approach on other text classification problems\n- **Learn More**: Explore advanced NLP techniques (transformers, embeddings)\n- **Build Portfolio**: Document this project for future employers\n- **Share Knowledge**: Teach others what you've learned!\n\n### üìö **Resources for Further Learning:**\n- **Books**: \"Hands-On Machine Learning\" by Aur√©lien G√©ron\n- **Courses**: Fast.ai, Coursera Machine Learning courses\n- **Libraries**: Explore spaCy, transformers, scikit-learn documentation\n- **Practice**: Kaggle competitions, personal projects\n\n**Remember**: The best way to learn data science is by doing projects like this one. Keep experimenting, keep learning, and most importantly - have fun with it! üöÄ\n\n---\n\n*Happy learning, future data scientist!* üìä‚ú®",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### üöÄ **Intermediate Challenges**\n\n1. **New Feature Creation**:\n   - Design and implement a new feature (e.g., exclamation mark count, average paragraph length)\n   - Add it to the feature extraction and test if it improves model performance\n\n2. **Model Improvement**:\n   - Try different Random Forest parameters (n_estimators, max_depth)\n   - Implement cross-validation to get more reliable performance estimates\n\n3. **Error Analysis**:\n   - Find examples where your model makes mistakes\n   - Analyze what makes these cases difficult to classify\n\n### üéì **Advanced Projects**\n\n1. **Ensemble Methods**:\n   - Combine multiple models (e.g., average their predictions)\n   - Research and implement a voting classifier\n\n2. **Deep Learning**:\n   - Research transformer models (BERT, RoBERTa) for text classification\n   - Compare traditional features vs. deep learning embeddings\n\n3. **Real-World Application**:\n   - Test your model on completely new text (from news websites, social media)\n   - Analyze where it succeeds and fails in the wild",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# EXERCISE WORKSPACE\n# Use this cell to experiment and answer the exercises above!\n\nprint(\"üí° EXERCISE WORKSPACE\")\nprint(\"=\" * 25)\nprint(\"Use this space to:\")\nprint(\"‚Ä¢ Experiment with different features\")\nprint(\"‚Ä¢ Test your understanding\") \nprint(\"‚Ä¢ Try the challenges below\")\nprint()\n\n# Example: Let's explore different articles\nif text_df is not None and len(text_df) > 0:\n    print(\"üîç EXERCISE 1: Explore different articles\")\n    print(\"Try changing this number to explore different examples:\")\n    \n    # TRY CHANGING THIS NUMBER (0 to len(text_df)-1)\n    explore_idx = 0\n    \n    if explore_idx < len(text_df):\n        sample = text_df.iloc[explore_idx]\n        print(f\"\\\\nArticle {sample['article_id']}:\")\n        print(f\"Real text length: {len(sample['real_text'])} chars\")\n        print(f\"Fake text length: {len(sample['fake_text'])} chars\")\n        print(f\"Real text preview: {sample['real_text'][:150]}...\")\n        print(f\"Fake text preview: {sample['fake_text'][:150]}...\")\n        \n        # Extract features for comparison\n        real_features = {**extract_basic_features(sample['real_text']), **extract_style_features(sample['real_text'])}\n        fake_features = {**extract_basic_features(sample['fake_text']), **extract_style_features(sample['fake_text'])}\n        \n        print(\"\\\\nüìä Feature comparison for this pair:\")\n        for feature in ['char_count', 'readability_score', 'sentiment_score']:\n            real_val = real_features.get(feature, 0)\n            fake_val = fake_features.get(feature, 0)\n            print(f\"  {feature}: Real={real_val:.3f}, Fake={fake_val:.3f}\")\n\nprint(\"\\\\nüéØ YOUR TURN:\")\nprint(\"1. Change 'explore_idx' to look at different articles\")\nprint(\"2. Notice patterns between real and fake text\")\nprint(\"3. Think about what features might help distinguish them\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## üéØ Hands-On Exercises\n\n### üî∞ **Beginner Exercises**\n\n1. **Feature Analysis**: \n   - Look at the feature importance from your model\n   - Pick the top 3 features and explain in plain English what they measure\n   - Do these features make intuitive sense for detecting AI text?\n\n2. **Data Exploration**:\n   - Try changing the `sample_idx` in the example viewing code\n   - Look at 3-4 different article pairs\n   - Can you spot patterns that might help distinguish real from fake text?\n\n3. **Model Comparison**:\n   - Compare the performance of Logistic Regression vs Random Forest\n   - Which model would you choose and why?",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def extract_advanced_features(text):\n    \"\"\"\n    ADVANCED: Extract more sophisticated text features.\n    \n    This function demonstrates advanced NLP concepts:\n    - N-gram analysis (word patterns)\n    - Part-of-speech analysis\n    - Named entity recognition\n    \"\"\"\n    if not text or len(text.strip()) == 0:\n        return {\n            'avg_word_freq': 0,\n            'complex_words_ratio': 0,\n            'sentence_variety': 0\n        }\n    \n    features = {}\n    words = text.lower().split()\n    \n    # 1. WORD FREQUENCY ANALYSIS\n    # Measure how \"common\" the vocabulary is\n    word_counts = Counter(words)\n    if words:\n        # Average frequency of words (higher = more repetitive)\n        features['avg_word_freq'] = sum(word_counts.values()) / len(word_counts)\n    else:\n        features['avg_word_freq'] = 0\n    \n    # 2. COMPLEXITY ANALYSIS\n    # Count \"complex\" words (more than 2 syllables - simplified)\n    complex_word_count = 0\n    for word in words:\n        # Simple syllable counting: count vowel groups\n        vowels = 'aeiouAEIOU'\n        syllables = len([char for i, char in enumerate(word) \n                        if char in vowels and (i == 0 or word[i-1] not in vowels)])\n        if syllables > 2:\n            complex_word_count += 1\n    \n    features['complex_words_ratio'] = complex_word_count / len(words) if words else 0\n    \n    # 3. SENTENCE VARIETY\n    # Measure diversity in sentence lengths\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if len(sentences) > 1:\n        sentence_lengths = [len(s.split()) for s in sentences]\n        features['sentence_variety'] = np.std(sentence_lengths) / np.mean(sentence_lengths) if np.mean(sentence_lengths) > 0 else 0\n    else:\n        features['sentence_variety'] = 0\n    \n    return features\n\n# Demonstrate advanced features (optional - only run if you want to explore)\nprint(\"üî¨ ADVANCED FEATURES DEMO\")\nprint(\"=\" * 30)\nprint(\"These are more sophisticated features you could implement:\")\nprint(\"\\\\n1. N-gram analysis - Look at word patterns (e.g., 'the quick brown')\")\nprint(\"2. Part-of-speech ratios - Analyze grammar patterns\")  \nprint(\"3. Named entity analysis - Count people, places, organizations\")\nprint(\"4. Sentiment progression - How sentiment changes through the text\")\nprint(\"5. Coherence measures - How well sentences connect to each other\")\nprint(\"\\\\nüí° Challenge: Try implementing one of these features!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## üöÄ Advanced Section (Optional)\n\n**Ready for more?** This section introduces advanced concepts for students who want to dive deeper into NLP and machine learning.\n\n### üí° Advanced Feature Ideas\n\nHere are some sophisticated features you could add to improve performance:",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## üéì Learning Checkpoint: What We've Accomplished\n\n**Congratulations!** You've just built your first text classification system! Let's review what we learned:\n\n### ‚úÖ **Key Concepts Mastered:**\n1. **Exploratory Data Analysis** - Understanding your data before modeling\n2. **Feature Engineering** - Converting text into numerical features\n3. **Train/Test Split** - Properly evaluating model performance\n4. **Model Training** - Using machine learning algorithms\n5. **Model Comparison** - Comparing different approaches\n6. **Model Interpretation** - Understanding what the model learned\n\n### üß† **Data Science Skills Developed:**\n- File handling and data loading\n- Text preprocessing and analysis\n- Statistical comparison of groups\n- Machine learning pipeline creation\n- Performance evaluation and interpretation\n\n### ü§î **Discussion Questions:**\n1. Which features were most important for distinguishing real from fake text?\n2. Why might Random Forest perform differently than Logistic Regression?\n3. What other features could we add to improve performance?\n4. How could we test our model on completely new types of text?\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "if 'X_train' in locals() and X_train is not None:\n    print(\"üå≥ Training Random Forest model...\")\n    \n    # Random Forest doesn't need scaled features (unlike Logistic Regression)\n    rf_model = RandomForestClassifier(\n        n_estimators=100,        # Number of trees in the forest\n        random_state=RANDOM_STATE,\n        max_depth=10,           # Prevent overfitting\n        min_samples_split=5     # Prevent overfitting\n    )\n    \n    # Train the model\n    rf_model.fit(X_train, y_train)\n    \n    # Make predictions\n    rf_train_pred = rf_model.predict(X_train)\n    rf_test_pred = rf_model.predict(X_test)\n    \n    # Calculate accuracies\n    rf_train_acc = accuracy_score(y_train, rf_train_pred)\n    rf_test_acc = accuracy_score(y_test, rf_test_pred)\n    \n    print(\"‚úÖ Random Forest training completed!\")\n    \n    print(f\"\\\\nüìä MODEL COMPARISON:\")\n    print(\"=\" * 40)\n    print(f\"{'Model':<20} {'Train Acc':<10} {'Test Acc':<10}\")\n    print(\"-\" * 40)\n    if 'train_accuracy' in locals():\n        print(f\"{'Logistic Regression':<20} {train_accuracy:<10.3f} {test_accuracy:<10.3f}\")\n    print(f\"{'Random Forest':<20} {rf_train_acc:<10.3f} {rf_test_acc:<10.3f}\")\n    \n    # Determine which model is better\n    print(f\"\\\\nüèÜ RESULTS:\")\n    if 'test_accuracy' in locals():\n        if rf_test_acc > test_accuracy:\n            improvement = (rf_test_acc - test_accuracy) * 100\n            print(f\"üéâ Random Forest wins! {improvement:.1f} percentage points better!\")\n        elif rf_test_acc < test_accuracy:\n            difference = (test_accuracy - rf_test_acc) * 100\n            print(f\"ü§î Logistic Regression was better by {difference:.1f} percentage points\")\n        else:\n            print(\"ü§ù Both models perform similarly!\")\n    else:\n        print(f\"Random Forest test accuracy: {rf_test_acc:.3f} ({rf_test_acc*100:.1f}%)\")\n        \n    # Check for overfitting\n    rf_overfit = rf_train_acc - rf_test_acc\n    if rf_overfit > 0.1:\n        print(f\"‚ö†Ô∏è Random Forest may be overfitting (gap: {rf_overfit:.3f})\")\n    else:\n        print(\"‚úÖ Random Forest shows good generalization\")\n        \nelse:\n    print(\"‚ùå Cannot train Random Forest - no training data available\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### üå≥ Let's Try a More Powerful Model: Random Forest\n\nRandom Forest is an **ensemble method** - it combines many decision trees to make better predictions. Let's see if we can improve our results!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "if 'model' in locals() and model is not None:\n    # Get feature importance from model coefficients\n    feature_importance = pd.DataFrame({\n        'feature': X_train.columns,\n        'coefficient': model.coef_[0],\n        'abs_coefficient': np.abs(model.coef_[0])\n    }).sort_values('abs_coefficient', ascending=False)\n    \n    print(\"üß† What our model learned (Feature Importance):\")\n    print(\"=\" * 60)\n    print(f\"{'Feature':<25} {'Coefficient':<12} {'Interpretation'}\")\n    print(\"-\" * 60)\n    \n    for _, row in feature_importance.head(8).iterrows():\n        feature = row['feature']\n        coef = row['coefficient']\n        \n        if coef > 0:\n            interpretation = \"Higher ‚Üí text_1 more likely real\"\n        else:\n            interpretation = \"Higher ‚Üí text_2 more likely real\"\n            \n        print(f\"{feature:<25} {coef:>+10.3f}  {interpretation}\")\n    \n    print(\"\\\\nüìñ How to read this:\")\n    print(\"‚Ä¢ Larger absolute coefficient = more important feature\")\n    print(\"‚Ä¢ Positive coefficient = when this difference is positive, text_1 is more likely real\")\n    print(\"‚Ä¢ Negative coefficient = when this difference is positive, text_2 is more likely real\")\n    \n    # Show the most important features\n    top_features = feature_importance.head(3)['feature'].tolist()\n    print(f\"\\\\nüéØ Top 3 most important features: {', '.join(top_features)}\")\n    \n    print(\"\\\\nü§î DISCUSSION QUESTIONS:\")\n    print(\"‚Ä¢ Do these important features make intuitive sense?\")\n    print(\"‚Ä¢ What do they tell us about the differences between real and AI text?\")\n    print(\"‚Ä¢ How might we improve our features based on these insights?\")\n    \nelse:\n    print(\"‚ùå No trained model available for analysis\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### üîç Understanding What Our Model Learned\n\nOne of the great things about Logistic Regression is that we can see which features it thinks are most important!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "if 'X_train' in locals() and X_train is not None:\n    print(\"üéì Training our first model...\")\n    \n    # Step 1: Scale the features (important for logistic regression)\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n    \n    print(\"üìè Features scaled (normalized to similar ranges)\")\n    \n    # Step 2: Create and train the model\n    model = LogisticRegression(random_state=RANDOM_STATE, max_iter=1000)\n    model.fit(X_train_scaled, y_train)\n    \n    print(\"‚úÖ Model training completed!\")\n    \n    # Step 3: Make predictions\n    train_predictions = model.predict(X_train_scaled)\n    test_predictions = model.predict(X_test_scaled)\n    \n    # Step 4: Calculate accuracy\n    train_accuracy = accuracy_score(y_train, train_predictions)\n    test_accuracy = accuracy_score(y_test, test_predictions)\n    \n    print(f\"\\\\nüìä MODEL PERFORMANCE:\")\n    print(f\"Training accuracy: {train_accuracy:.3f} ({train_accuracy*100:.1f}%)\")\n    print(f\"Test accuracy: {test_accuracy:.3f} ({test_accuracy*100:.1f}%)\")\n    \n    # Step 5: Interpret the results\n    print(f\"\\\\nü§î INTERPRETATION:\")\n    if test_accuracy > 0.8:\n        print(\"üéâ Excellent! Our model is performing very well!\")\n    elif test_accuracy > 0.7:\n        print(\"üòä Good performance! There's room for improvement.\")\n    elif test_accuracy > 0.6:\n        print(\"üòê Moderate performance. We need better features or models.\")\n    else:\n        print(\"üòî Poor performance. Back to the drawing board!\")\n    \n    # Check for overfitting\n    if train_accuracy - test_accuracy > 0.1:\n        print(\"‚ö†Ô∏è Warning: Model might be overfitting (much better on training than test)\")\n    else:\n        print(\"‚úÖ Good: Similar performance on training and test data\")\n        \n    print(f\"\\\\nüéØ Random guessing would give ~50% accuracy. Our model: {test_accuracy*100:.1f}%\")\n    \nelse:\n    print(\"‚ùå Cannot train model - no training data available\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### ü§ñ Training Our First Model\n\nLet's start with a simple but effective algorithm: **Logistic Regression**\n\n**Why Logistic Regression?**\n- Easy to understand and interpret\n- Fast to train\n- Good baseline for text classification\n- Shows us which features are most important",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "if X is not None and y is not None:\n    # Split data: 80% for training, 20% for testing\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, \n        test_size=0.2,           # 20% for testing\n        random_state=RANDOM_STATE,  # For reproducible results\n        stratify=y               # Keep same proportion of classes in both splits\n    )\n    \n    print(\"üìä Data Split Results:\")\n    print(f\"Training set: {len(X_train)} samples\")\n    print(f\"Test set: {len(X_test)} samples\")\n    print(f\"Training labels: {np.bincount(y_train)}\")\n    print(f\"Test labels: {np.bincount(y_test)}\")\n    \n    print(\"\\\\nüí° Why this split matters:\")\n    print(\"‚Ä¢ Training data: Model learns patterns from this\")\n    print(\"‚Ä¢ Test data: We evaluate performance on this (model never sees it during training)\")\n    print(\"‚Ä¢ Stratify: Ensures both sets have similar class distributions\")\n    \nelse:\n    print(\"‚ùå No data available for splitting\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### üîÑ Splitting Our Data\n\n**Why do we split data?** We need to test our model on data it hasn't seen during training. This tells us how well it will perform on new, unseen articles.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def prepare_ml_data(text_df):\n    \"\"\"\n    Prepare data for machine learning.\n    \n    For each article pair, we need to create features and a label:\n    - Features: Differences between text_1 and text_2 \n    - Label: Which text is real (1 = text_1 is real, 0 = text_2 is real)\n    \"\"\"\n    print(\"üõ†Ô∏è Preparing data for machine learning...\")\n    \n    if text_df is None or len(text_df) == 0:\n        print(\"‚ùå No data to prepare\")\n        return None, None\n    \n    X = []  # Features (input to our model)\n    y = []  # Labels (what we want to predict)\n    \n    for idx, row in text_df.iterrows():\n        # Extract features for both texts\n        basic_feat_1 = extract_basic_features(row['text_1'])\n        style_feat_1 = extract_style_features(row['text_1'])\n        \n        basic_feat_2 = extract_basic_features(row['text_2']) \n        style_feat_2 = extract_style_features(row['text_2'])\\n        \n        # Combine features\n        feat_1 = {**basic_feat_1, **style_feat_1}\n        feat_2 = {**basic_feat_2, **style_feat_2}\n        \n        # Create difference features (this is key!)\\n        # Instead of absolute values, we look at differences between the texts\n        feature_vector = {}\n        for key in feat_1.keys():\n            feature_vector[f'{key}_diff'] = feat_1[key] - feat_2[key]\n            \n        X.append(feature_vector)\n        \n        # Label: 1 if text_1 is real, 0 if text_2 is real\n        label = 1 if row['real_text_id'] == 1 else 0\n        y.append(label)\n        \n        # Show progress\n        if (idx + 1) % 10 == 0:\n            print(f\"  Processed {idx + 1}/{len(text_df)} articles...\")\n    \n    # Convert to arrays for scikit-learn\n    X_df = pd.DataFrame(X)\n    y_array = np.array(y)\n    \n    print(f\"‚úÖ Prepared {len(X_df)} samples with {len(X_df.columns)} features\")\n    print(f\"üìä Class distribution: {np.bincount(y_array)} (0=text_2 real, 1=text_1 real)\")\n    \n    return X_df, y_array\n\n# Prepare our machine learning data\nif text_df is not None:\n    X, y = prepare_ml_data(text_df)\n    \n    if X is not None:\n        print(f\"\\\\nüîç Feature names: {list(X.columns)}\")\n        print(f\"\\\\nüìà Example feature vector (first sample):\")\n        for feature, value in X.iloc[0].items():\n            print(f\"  {feature}: {value:.3f}\")\n            \n        print(f\"\\\\nüéØ Corresponding label: {y[0]} ({'text_1 is real' if y[0] == 1 else 'text_2 is real'})\")\nelse:\n    print(\"‚ùå No text data available\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Step 4: Building Our First Machine Learning Model\n\nNow comes the exciting part - let's use machine learning to automatically classify text as real or fake!\n\n### üéØ The Machine Learning Process\n\n1. **Prepare the data** - Combine features from both texts in each pair\n2. **Split the data** - Keep some data for testing our model\n3. **Train the model** - Let the algorithm learn patterns\n4. **Evaluate performance** - How accurate is our model?\n\n### üìä Preparing Our Training Data",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def extract_style_features(text):\n    \"\"\"\n    Extract features that capture writing style and quality.\n    These go beyond simple counting to analyze HOW the text is written.\n    \"\"\"\n    if not text or len(text.strip()) == 0:\n        return {\n            'readability_score': 0,\n            'sentiment_score': 0,\n            'punctuation_ratio': 0,\n            'capital_ratio': 0,\n            'unique_word_ratio': 0\n        }\n    \n    features = {}\n    \n    # 1. READABILITY - How easy is the text to read?\n    try:\n        features['readability_score'] = flesch_reading_ease(text)\n    except:\n        features['readability_score'] = 0\n    \n    # 2. SENTIMENT - What's the emotional tone?\n    sia = SentimentIntensityAnalyzer()\n    sentiment = sia.polarity_scores(text)\n    features['sentiment_score'] = sentiment['compound']  # Overall sentiment (-1 to +1)\n    \n    # 3. PUNCTUATION - How much punctuation is used?\n    punctuation_count = sum(1 for char in text if char in '.,!?;:-')\n    features['punctuation_ratio'] = punctuation_count / len(text) if len(text) > 0 else 0\n    \n    # 4. CAPITALIZATION - How much text is capitalized?\n    capital_count = sum(1 for char in text if char.isupper())\n    features['capital_ratio'] = capital_count / len(text) if len(text) > 0 else 0\n    \n    # 5. VOCABULARY DIVERSITY - How varied is the word choice?\n    words = text.lower().split()\n    if words:\n        unique_words = len(set(words))\n        features['unique_word_ratio'] = unique_words / len(words)\n    else:\n        features['unique_word_ratio'] = 0\n    \n    return features\n\n# Test the new features\nif text_df is not None and len(text_df) > 0:\n    sample_text = text_df.iloc[0]['real_text']\n    style_features = extract_style_features(sample_text)\n    \n    print(\"üé® Testing style features:\")\n    print(\"Text preview:\", sample_text[:100] + \"...\")\n    print(\"\\nStyle features:\")\n    for feature_name, value in style_features.items():\n        print(f\"‚Ä¢ {feature_name}: {value:.3f}\")\n        \n    print(\"\\nüìñ What these features mean:\")\n    print(\"‚Ä¢ readability_score: Higher = easier to read (0-100 scale)\")\n    print(\"‚Ä¢ sentiment_score: Emotional tone (-1 = negative, +1 = positive)\")\n    print(\"‚Ä¢ punctuation_ratio: Fraction of text that's punctuation\")\n    print(\"‚Ä¢ capital_ratio: Fraction of text that's uppercase\")\n    print(\"‚Ä¢ unique_word_ratio: Vocabulary diversity (1 = all words unique)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### üìà Adding More Sophisticated Features\n\nBasic length features are a good start, but we can do better! Let's add features that capture writing style and quality:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "if text_df is not None and len(text_df) > 0:\n    # Extract features for all texts\n    print(\"üîÑ Extracting features for all texts...\")\n    \n    real_features = []\n    fake_features = []\n    \n    for idx, row in text_df.iterrows():\n        real_feat = extract_basic_features(row['real_text'])\n        fake_feat = extract_basic_features(row['fake_text'])\n        \n        real_features.append(real_feat)\n        fake_features.append(fake_feat)\n    \n    # Convert to DataFrames for easy analysis\n    real_df = pd.DataFrame(real_features)\n    fake_df = pd.DataFrame(fake_features)\n    \n    print(f\"‚úÖ Extracted features for {len(real_df)} real and {len(fake_df)} fake texts\")\n    \n    # Compare averages\n    print(\"\\nüìä FEATURE COMPARISON (Averages)\")\n    print(\"=\" * 50)\n    print(f\"{'Feature':<20} {'Real Text':<12} {'Fake Text':<12} {'Difference'}\")\n    print(\"-\" * 50)\n    \n    for feature in real_df.columns:\n        real_avg = real_df[feature].mean()\n        fake_avg = fake_df[feature].mean()\n        diff = real_avg - fake_avg\n        diff_pct = (diff / fake_avg * 100) if fake_avg != 0 else 0\n        \n        print(f\"{feature:<20} {real_avg:<12.1f} {fake_avg:<12.1f} {diff:+6.1f} ({diff_pct:+.1f}%)\")\n    \n    print(\"\\nü§î INTERPRETATION:\")\n    print(\"‚Ä¢ Positive difference = Real texts have higher values\")\n    print(\"‚Ä¢ Negative difference = Fake texts have higher values\") \n    print(\"‚Ä¢ Look for large percentage differences!\")\n    \n    # Find the most discriminative features\n    differences = []\n    for feature in real_df.columns:\n        real_avg = real_df[feature].mean()\n        fake_avg = fake_df[feature].mean()\n        if fake_avg != 0:\n            diff_pct = abs((real_avg - fake_avg) / fake_avg * 100)\n            differences.append((feature, diff_pct))\n    \n    differences.sort(key=lambda x: x[1], reverse=True)\n    \n    print(f\"\\nüéØ Most discriminative features:\")\n    for feature, diff_pct in differences[:3]:\n        print(f\"‚Ä¢ {feature}: {diff_pct:.1f}% difference\")\n        \nelse:\n    print(\"‚ùå No data available for feature extraction\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### üìä Let's Compare Real vs Fake Features\n\nNow let's see if these basic features can help us distinguish real from fake text:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def extract_basic_features(text):\n    \"\"\"\n    Extract simple, interpretable features from text.\n    \n    Returns a dictionary of feature_name: value pairs.\n    \"\"\"\n    if not text or len(text.strip()) == 0:\n        # Handle empty text gracefully\n        return {\n            'char_count': 0,\n            'word_count': 0, \n            'sentence_count': 0,\n            'avg_word_length': 0,\n            'avg_sentence_length': 0\n        }\n    \n    features = {}\n    \n    # Basic counts\n    features['char_count'] = len(text)\n    features['word_count'] = len(text.split())\n    \n    # Count sentences (simple approach - count periods, exclamations, questions)\n    sentence_endings = text.count('.') + text.count('!') + text.count('?')\n    features['sentence_count'] = max(1, sentence_endings)  # At least 1 sentence\n    \n    # Average lengths\n    words = text.split()\n    if words:\n        features['avg_word_length'] = sum(len(word) for word in words) / len(words)\n    else:\n        features['avg_word_length'] = 0\n        \n    features['avg_sentence_length'] = features['word_count'] / features['sentence_count']\n    \n    return features\n\n# Test our function on one example\nif text_df is not None and len(text_df) > 0:\n    sample_text = text_df.iloc[0]['real_text']\n    sample_features = extract_basic_features(sample_text)\n    \n    print(\"üß™ Testing our feature extraction:\")\n    print(\"Text preview:\", sample_text[:100] + \"...\")\n    print(\"\\nExtracted features:\")\n    for feature_name, value in sample_features.items():\n        print(f\"‚Ä¢ {feature_name}: {value:.1f}\")\n        \n    print(\"\\nüí° These numbers represent measurable properties of the text!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### üìè Simple Features: Length and Structure\n\nLet's start with the most obvious features - how long are the texts and how are they structured?",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Step 3: Feature Engineering - Converting Text to Numbers\n\n**The Challenge**: Machine learning algorithms need numbers, but we have text. How do we convert text into meaningful features?\n\n**Feature Engineering** is the process of extracting measurable properties from raw data. For text, we might look at:\n- Length and structure\n- Readability and complexity  \n- Sentiment and tone\n- Writing patterns\n\nLet's start simple and build up our understanding!",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}